# Regular SPLADE Training Configuration — H100 GPU Optimized (NO ROUTER)
# Uses same MLM transformer for both queries and documents
# Better accuracy, slightly slower query inference
# Optimized for NVIDIA H100 NVL (96GB VRAM, CUDA 12.2, Compute Capability 9.0)
# ============================================================================
# MODEL CONFIGURATION
# ============================================================================
model:
  base_model: "ltg/norbert4-base"
  architecture: "splade"
  pooling_strategy: "max"
  use_router: false
  # Patch NorBERT4's sigmoid activation (30 * sigmoid(x/7.5)) for proper SPLADE sparsity.
  # Without this, all logits are forced to (0, 30) → ReLU can never produce exact zeros.
  # With this, raw logits range from ~-100 to +25 → SPLADE achieves true sparsity.
  patch_sigmoid_for_sparsity: true

# ============================================================================
# TRAINING CONFIGURATION
# ============================================================================
training:
  output_dir: "models/splade-norbert4-base-regular-multidataset"
  
  # Training duration
  num_train_epochs: 1
  
  # Batch sizes — norbert4-large (355M) safe for single H100
  # Memory: 355M params (~800MB) + ~5KB per token per sample
  # 512 tokens × 5KB × batch=8 ≈ ~25GB on 96GB H100 — safe margin
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 1
  
  # Learning rate
  # Official sbert MS MARCO SPLADE example uses 2e-5 with distilbert (30K vocab).
  # We keep 2e-5 — this is standard and well-tested for SPLADE from scratch.
  # The real problem was not LR but regularizer weights being too weak for 51K vocab.
  learning_rate: 0.00002  # 2e-5
  
  # Optimization
  # LR warmup_ratio=0.1 is standard per sbert examples. The regularizer has its
  # own separate warmup (quadratic over first 1/3 of training) handled by
  # SpladeRegularizerWeightSchedulerCallback. These are independent schedules.
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Precision — H100 has hardware BF16 Tensor Cores
  fp16: false
  bf16: true
  
  # Torch compile — disabled due to variable sequence lengths causing recompilation
  torch_compile: false
  
  # Sequence length — NorBERT4-large trained on 16384 tokens
  # Using 1024 for safe single H100 training, can go to 2048+ on multi-H100
  max_seq_length: 4096
  
  # Batch sampling
  # Note: "no_duplicates" is slow to initialize with large datasets (1M+ samples)
  # as it scans all samples before yielding the first batch.
  # Use "batch_sampler" for faster startup, or keep "no_duplicates" and be patient.
  batch_sampler: "batch_sampler"
  
  # Evaluation
  eval_strategy: "steps"  # Evaluate every eval_steps
  eval_steps: 1000  # Evaluate every 1000 steps
  logging_steps: 100
  
  # Early stopping — DISABLED for this run
  # The SpladeRegularizerWeightSchedulerCallback ramps regularization over the
  # first 1/3 of training (~7.5K steps). This causes a temporary NDCG dip as the
  # model learns to be sparse. With only 1 epoch, early stopping would kill
  # training before the model can recover. The official sbert MS MARCO example
  # also does NOT use early stopping.
  # Re-enable after establishing a working baseline with proper sparsity.
  # early_stopping_patience: 10
  # early_stopping_threshold: 0.001
  
  # Saving
  save_strategy: "steps"
  save_steps: 1000  # Save checkpoint every 1000 steps (same as eval)
  save_total_limit: 5  # Keep last 5 checkpoints (load_best_model_at_end needs the best one kept)
  load_best_model_at_end: true  # Load best model at end of training
  metric_for_best_model: "NanoBEIR_mean_dot_ndcg@10"  # Metric from SparseNanoBEIREvaluator
  greater_is_better: true  # Higher NDCG is better
  
  # Push to HuggingFace Hub
  push_to_hub: true
  hub_model_id: "thivy/norbert4-base-splade"
  hub_strategy: "checkpoint"
  hub_private_repo: false
  
  # Logging
  run_name: "splade-norbert4-base-regular-multidataset"
  report_to: ["tensorboard", "mlflow"]
  
  # Data loading — reduce workers for DDP to avoid slow sampler init
  # With no_duplicates sampler + large datasets, fewer workers = faster startup
  dataloader_num_workers: 2
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 2

# ============================================================================
# MLFLOW CONFIGURATION
# ============================================================================
mlflow:
  tracking_uri: "mlruns"
  experiment_name: "sparse-encoder-scandinavian"
  tags:
    project: "scandinavian-sparse-embeddings"
    framework: "sentence-transformers"
    architecture: "regular-splade"

# ============================================================================
# LOSS CONFIGURATION
# ============================================================================
loss:
  type: "splade_loss"
  inner_loss: "sparse_multiple_negatives_ranking_loss"
  # Regularization weights control sparsity (higher = sparser embeddings)
  # The SpladeRegularizerWeightSchedulerCallback (auto-added by trainer) ramps
  # these from 0 to max over the first 1/3 of training (quadratic schedule).
  #
  # Official sbert examples use these values:
  #   MS MARCO retrieval: query=5e-5, doc=1e-3 (distilbert, 30K vocab)
  #   NLI finetuning:     doc=3e-3 (with use_document_regularizer_only)
  #
  # Previous run with query=5e-5, doc=1e-3 produced ZERO sparsity on NorBERT4
  # (51K vocab) — all 51,200 dims remained active throughout training.
  # FlopsLoss = sum(mean(embedding, dim=0)^2), so larger vocab = more terms
  # in the sum = need stronger weight to push individual dims to zero.
  # Using 3e-3 for documents, aligned with the NLI example.
  query_regularizer_weight: 0.0001   # 1e-4 (up from 5e-5)
  document_regularizer_weight: 0.003  # 3e-3 (up from 1e-3, matches sbert NLI example)

# ============================================================================
# DATASET CONFIGURATION
# ============================================================================
datasets:
  sampling_strategy: "round_robin"
  
  nli:
    enabled: true
    source: "Fremtind/all-nli-norwegian"
    languages: ["norwegian"]
    split_ratio: [0.98, 0.01, 0.01]
  
  scandi_qa:
    enabled: true
    use_norquad: true
    use_openbookqa: true
    use_scandiqa: true
    use_supervised_da: true
    use_paws: true
    scandiqa_languages: ["no", "da", "sv"]
    paws_data_dir: "data/paws-x/x-final"
  
  ddsc:
    enabled: true
    languages: ["norwegian", "danish", "swedish"]
    tasks: null
    split_ratio: [0.98, 0.01, 0.01]
  
  eti:
    enabled: false  # ETI dataset from official health/welfare information
    source: "thivy/eti-embedding-training-data"
    split_ratio: [0.98, 0.01, 0.01]

# ============================================================================
# EVALUATION CONFIGURATION
# ============================================================================
evaluation:
  evaluator: "sparse_nanobeir"  # Evaluator type: sparse_nanobeir for sparse embeddings
  nanobeir_datasets:
    - "nfcorpus"  # Small benchmark — lightweight eval during training

# ============================================================================
# MODEL CARD CONFIGURATION
# ============================================================================
model_card:
  language: ["no", "da", "sv"]
  license: "mit"
  library_name: "sentence-transformers"
  tags:
    - "sparse-encoder"
    - "splade"
    - "regular-splade"
    - "norwegian"
    - "danish"
    - "swedish"
  model_name: "Regular SPLADE NorBERT4-large trained on Multi-dataset Norwegian/Scandinavian"
  datasets_used:
    - "Fremtind/all-nli-norwegian"
    - "NorQuAD"
    - "OpenBookQA"
    - "ScandiQA"
    - "DDSC Nordic Embedding Training Data"
    - "PAWS-X Norwegian"
    - "ETI (Elektronisk Tjenesteinformasjon)"

# ============================================================================
# HARDWARE CONFIGURATION — NVIDIA H100 NVL
# ============================================================================
hardware:
  device: "cuda"
  gradient_checkpointing: true  # Essential for SPLADE — saves ~60% activation memory
  tf32: true
  distributed: false
  world_size: 1

# NOTE: With evaluation enabled, VRAM usage increases during eval_steps:
# - Training: ~55-60 GB per GPU
# - Evaluation: +15-20 GB for loading benchmark datasets and running inference
# - Total at eval time: ~75-80 GB per GPU (still within 96GB H100 limit with margin)
