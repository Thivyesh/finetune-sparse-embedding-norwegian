# Regular SPLADE Training Configuration — H100 GPU Optimized (NO ROUTER)
# Uses same MLM transformer for both queries and documents
# Better accuracy, slightly slower query inference
# Optimized for NVIDIA H100 NVL (96GB VRAM, CUDA 12.2, Compute Capability 9.0)
# ============================================================================
# MODEL CONFIGURATION
# ============================================================================
model:
  base_model: "ltg/norbert4-base"
  architecture: "splade"
  pooling_strategy: "max"
  use_router: false

# ============================================================================
# TRAINING CONFIGURATION
# ============================================================================
training:
  output_dir: "models/splade-norbert4-base-regular-multidataset"
  
  # Training duration
  num_train_epochs: 1
  
  # Batch sizes — norbert4-large (355M) safe for single H100
  # Memory: 355M params (~800MB) + ~5KB per token per sample
  # 512 tokens × 5KB × batch=8 ≈ ~25GB on 96GB H100 — safe margin
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 1
  
  # Learning rate
  learning_rate: 0.00002  # 2e-5
  
  # Optimization
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Precision — H100 has hardware BF16 Tensor Cores
  fp16: false
  bf16: true
  
  # Torch compile — disabled due to variable sequence lengths causing recompilation
  torch_compile: false
  
  # Sequence length — NorBERT4-large trained on 16384 tokens
  # Using 1024 for safe single H100 training, can go to 2048+ on multi-H100
  max_seq_length: 4096
  
  # Batch sampling
  # Note: "no_duplicates" is slow to initialize with large datasets (1M+ samples)
  # as it scans all samples before yielding the first batch.
  # Use "batch_sampler" for faster startup, or keep "no_duplicates" and be patient.
  batch_sampler: "batch_sampler"
  
  # Evaluation
  eval_strategy: "steps"  # Evaluate every eval_steps
  eval_steps: 1000  # Evaluate every 1000 steps
  logging_steps: 100
  
  # Early stopping
  early_stopping_patience: 3  # Stop if no improvement for 3 evaluations
  early_stopping_threshold: 0.001  # Minimum improvement threshold
  
  # Saving
  save_strategy: "steps"
  save_steps: 1000  # Save checkpoint every 1000 steps (same as eval)
  save_total_limit: 3  # Keep last 3 checkpoints (need multiple for early stopping)
  load_best_model_at_end: true  # Load best model at end of training
  metric_for_best_model: "NanoBEIR_mean_dot_ndcg@10"  # Metric from SparseNanoBEIREvaluator
  greater_is_better: true  # Higher NDCG is better
  
  # Push to HuggingFace Hub
  push_to_hub: true
  hub_model_id: "thivy/norbert4-base-splade"
  hub_strategy: "checkpoint"
  hub_private_repo: false
  
  # Logging
  run_name: "splade-norbert4-base-regular-multidataset"
  report_to: ["tensorboard", "mlflow"]
  
  # Data loading — reduce workers for DDP to avoid slow sampler init
  # With no_duplicates sampler + large datasets, fewer workers = faster startup
  dataloader_num_workers: 2
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 2

# ============================================================================
# MLFLOW CONFIGURATION
# ============================================================================
mlflow:
  tracking_uri: "mlruns"
  experiment_name: "sparse-encoder-scandinavian"
  tags:
    project: "scandinavian-sparse-embeddings"
    framework: "sentence-transformers"
    architecture: "regular-splade"

# ============================================================================
# LOSS CONFIGURATION
# ============================================================================
loss:
  type: "splade_loss"
  inner_loss: "sparse_multiple_negatives_ranking_loss"
  # Regularization weights control sparsity (higher = sparser embeddings)
  # The SpladeRegularizerWeightSchedulerCallback (auto-added by trainer) ramps
  # these from 0 to max over the first 1/3 of training (quadratic schedule).
  # Values based on SBERT MS MARCO MNRL example (asymmetric retrieval):
  query_regularizer_weight: 0.00005  # 5e-5 — light query sparsity (queries are short)
  document_regularizer_weight: 0.001  # 1e-3 — stronger document sparsity (docs need pruning)

# ============================================================================
# DATASET CONFIGURATION
# ============================================================================
datasets:
  sampling_strategy: "round_robin"
  
  nli:
    enabled: true
    source: "Fremtind/all-nli-norwegian"
    languages: ["norwegian"]
    split_ratio: [0.98, 0.01, 0.01]
  
  scandi_qa:
    enabled: true
    use_norquad: true
    use_openbookqa: true
    use_scandiqa: true
    use_supervised_da: true
    use_paws: true
    scandiqa_languages: ["no", "da", "sv"]
    paws_data_dir: "data/paws-x/x-final"
  
  ddsc:
    enabled: true
    languages: ["norwegian", "danish", "swedish"]
    tasks: null
    split_ratio: [0.98, 0.01, 0.01]

# ============================================================================
# EVALUATION CONFIGURATION
# ============================================================================
evaluation:
  evaluator: "sparse_nanobeir"  # Evaluator type: sparse_nanobeir for sparse embeddings
  nanobeir_datasets:
    - "nfcorpus"  # Small benchmark — lightweight eval during training

# ============================================================================
# MODEL CARD CONFIGURATION
# ============================================================================
model_card:
  language: ["no", "da", "sv"]
  license: "mit"
  library_name: "sentence-transformers"
  tags:
    - "sparse-encoder"
    - "splade"
    - "regular-splade"
    - "norwegian"
    - "danish"
    - "swedish"
  model_name: "Regular SPLADE NorBERT4-large trained on Multi-dataset Norwegian/Scandinavian"
  datasets_used:
    - "Fremtind/all-nli-norwegian"
    - "NorQuAD"
    - "OpenBookQA"
    - "ScandiQA"
    - "DDSC Nordic Embedding Training Data"
    - "PAWS-X Norwegian"

# ============================================================================
# HARDWARE CONFIGURATION — NVIDIA H100 NVL
# ============================================================================
hardware:
  device: "cuda"
  gradient_checkpointing: true  # Essential for SPLADE — saves ~60% activation memory
  tf32: true
  distributed: false
  world_size: 1

# NOTE: With evaluation enabled, VRAM usage increases during eval_steps:
# - Training: ~55-60 GB per GPU
# - Evaluation: +15-20 GB for loading benchmark datasets and running inference
# - Total at eval time: ~75-80 GB per GPU (still within 96GB H100 limit with margin)
