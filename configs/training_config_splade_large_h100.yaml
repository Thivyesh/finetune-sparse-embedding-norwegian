# Sparse Encoder Training Configuration — LARGE MODEL, H100 GPU Optimized
# Inference-free SPLADE with ltg/norbert4-large for production deployment
# Optimized for NVIDIA H100 NVL (96GB VRAM, CUDA 12.2, Compute Capability 9.0)

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================
model:
  base_model: "ltg/norbert4-large"  # 355M params, 768 dim
  architecture: "inference-free-splade"
  pooling_strategy: "max"
  use_router: true
  freeze_static_embedding: false

# ============================================================================
# TRAINING CONFIGURATION
# ============================================================================
training:
  output_dir: "models/splade-norbert4-large-multidataset"
  
  # Training duration
  num_train_epochs: 1
  max_steps: -1
  
  # Batch sizes — norbert4-large (355M) safe for single H100
  # Memory: 355M params (~800MB) + ~5KB per token per sample
  # 512 tokens × 5KB × batch=16 ≈ ~40GB on 96GB H100 — comfortable
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 1  # No accumulation needed on H100
  
  # Learning rates
  learning_rate: 0.00002  # 2e-5
  query_learning_rate: 0.001  # 1e-3
  
  # Optimization
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Precision — H100 native BF16 Tensor Cores
  fp16: false
  bf16: true
  
  # Torch compile for H100 kernel optimization
  # Disabled due to variable sequence lengths causing recompilation overhead
  torch_compile: false
  
  # Sequence length — NorBERT4-large trained on 16384 tokens
  # Using 512 for safe single H100 training, can go to 1024+ on multi-H100
  max_seq_length: 512
  
  # Batch sampling
  batch_sampler: "no_duplicates"
  
  # Evaluation strategy
  eval_strategy: "steps"
  eval_steps: 2000
  logging_steps: 200
  
  # Saving strategy
  save_strategy: "steps"
  save_steps: 1000
  save_total_limit: 1
  load_best_model_at_end: true
  metric_for_best_model: "eval_NanoBEIR_mean_dot_ndcg@10"
  greater_is_better: true
  
  # Push to HuggingFace Hub
  push_to_hub: true
  hub_model_id: "thivy/splade-norbert4-scandinavian-multidataset"
  hub_strategy: "checkpoint"
  hub_private_repo: false
  
  # Logging
  run_name: "splade-norbert4-large-multidataset"
  report_to: ["tensorboard", "mlflow"]
  
  # Data loading — CUDA multi-process data loading
  dataloader_num_workers: 8
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 4

# ============================================================================
# MLFLOW CONFIGURATION
# ============================================================================
mlflow:
  tracking_uri: "mlruns"
  experiment_name: "sparse-encoder-scandinavian-large"
  tags:
    project: "scandinavian-sparse-embeddings"
    framework: "sentence-transformers"
    model_size: "large"

# ============================================================================
# LOSS CONFIGURATION
# ============================================================================
loss:
  type: "splade_loss"
  inner_loss: "sparse_multiple_negatives_ranking_loss"
  query_regularizer_weight: 0
  document_regularizer_weight: 0.003

# ============================================================================
# DATASET CONFIGURATION
# ============================================================================
datasets:
  sampling_strategy: "round_robin"
  
  nli:
    enabled: true
    source: "Fremtind/all-nli-norwegian"
    languages: ["norwegian"]
    split_ratio: [0.98, 0.01, 0.01]
  
  scandi_qa:
    enabled: true
    use_norquad: true
    use_openbookqa: true
    use_scandiqa: true
    use_supervised_da: true
    use_paws: true
    scandiqa_languages: ["no", "da", "sv"]
    paws_data_dir: "data/paws-x/x-final"
  
  ddsc:
    enabled: true
    languages: ["norwegian", "danish", "swedish"]
    tasks: null
    split_ratio: [0.98, 0.01, 0.01]

# ============================================================================
# EVALUATION CONFIGURATION
# ============================================================================
evaluation:
  evaluator: "sparse_nanobeir"
  nanobeir_datasets: ["msmarco", "nfcorpus", "nq"]
  mteb_tasks: ["NorQuAD", "SNL"]
  track_sparsity: true
  log_decoded_samples: 10

# ============================================================================
# INFERENCE-FREE SPLADE SPECIFIC
# ============================================================================
router:
  mapping:
    anchor: "query"
    positive: "document"
    negative: "document"
  learning_rate_patterns:
    ".*\\.query\\..*": 0.001

# ============================================================================
# MODEL CARD CONFIGURATION
# ============================================================================
model_card:
  language: ["no", "da", "sv"]
  license: "mit"
  library_name: "sentence-transformers"
  tags:
    - "sparse-encoder"
    - "splade"
    - "norwegian"
    - "danish"
    - "swedish"
    - "scandinavian"
    - "multilingual"
    - "information-retrieval"
  model_name: "Inference-free SPLADE NorBERT4-large trained on Multi-dataset Norwegian/Scandinavian"
  datasets_used:
    - "Fremtind/all-nli-norwegian"
    - "NorQuAD"
    - "OpenBookQA"
    - "ScandiQA"
    - "DDSC Nordic Embedding Training Data"
    - "PAWS-X Norwegian"

# ============================================================================
# HARDWARE CONFIGURATION — NVIDIA H100 NVL
# ============================================================================
hardware:
  device: "cuda"
  gradient_checkpointing: false  # Not needed — 96GB VRAM handles norbert4-base easily
  tf32: true  # H100 TF32 Tensor Cores for 3x faster FP32 matmuls
  distributed: false
  world_size: 1
