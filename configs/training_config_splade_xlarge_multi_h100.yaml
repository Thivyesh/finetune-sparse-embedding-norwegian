# Sparse Encoder Training Configuration — MULTI-H100 + XLARGE MODEL
# Inference-free SPLADE with ltg/norbert4-xlarge for maximum quality
# Optimized for 4-8 NVIDIA H100 NVL GPUs (96GB VRAM each)
# Use with: deepspeed launch or torch.distributed.launch
# ============================================================================
# MODEL CONFIGURATION
# ============================================================================
model:
  base_model: "ltg/norbert4-xlarge"  # 987M params, 1536 hidden dim
  architecture: "inference-free-splade"
  pooling_strategy: "max"
  use_router: true
  freeze_static_embedding: false

# ============================================================================
# TRAINING CONFIGURATION — AGGRESSIVE MULTI-GPU
# ============================================================================
training:
  output_dir: "models/splade-norbert4-xlarge-multidataset-multi-h100"
  
  # Training duration
  num_train_epochs: 2
  max_steps: -1
  
  # Batch sizes — norbert4-xlarge (987M) with 4x H100s
  # Per GPU: batch=16, global batch = 16*4=64
  # XLarge model ~2.3GB + 512 tokens × 5KB × batch=16 ≈ ~80GB per GPU
  # Works well with distributed training and gradient sync
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 32
  gradient_accumulation_steps: 1
  
  # Learning rates
  learning_rate: 0.00002  # 2e-5 (lower for larger model stability)
  query_learning_rate: 0.001  # 1e-3
  
  # Optimization
  warmup_ratio: 0.15  # Longer warmup for XLarge
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Precision — H100 native BF16
  fp16: false
  bf16: true
  
  # Torch compile disabled
  torch_compile: false
  
  # Sequence length — XLarge trained on 16384, using 2048 with distributed training
  # Much longer context for quality improvement
  max_seq_length: 2048
  
  # Batch sampling
  batch_sampler: "no_duplicates"
  
  # Evaluation strategy
  eval_strategy: "steps"
  eval_steps: 1000
  logging_steps: 100
  
  # Saving strategy
  save_strategy: "steps"
  save_steps: 500
  save_total_limit: 3
  load_best_model_at_end: true
  metric_for_best_model: "eval_NanoBEIR_mean_dot_ndcg@10"
  greater_is_better: true
  
  # Push to HuggingFace Hub
  push_to_hub: true
  hub_model_id: "thivy/splade-norbert4-xlarge-multidataset"
  hub_strategy: "checkpoint"
  hub_private_repo: false
  
  # Logging
  run_name: "splade-norbert4-xlarge-multidataset-multi-h100"
  report_to: ["tensorboard", "mlflow"]
  
  # Data loading
  dataloader_num_workers: 8
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 4

# ============================================================================
# MLFLOW CONFIGURATION
# ============================================================================
mlflow:
  tracking_uri: "mlruns"
  experiment_name: "sparse-encoder-scandinavian-xlarge-multi-gpu"
  tags:
    project: "scandinavian-sparse-embeddings"
    framework: "sentence-transformers"
    model_size: "xlarge"
    training_setup: "multi-h100"
    context_length: "2048"

# ============================================================================
# LOSS CONFIGURATION
# ============================================================================
loss:
  type: "splade_loss"
  inner_loss: "sparse_multiple_negatives_ranking_loss"
  query_regularizer_weight: 0
  document_regularizer_weight: 0.003

# ============================================================================
# DATASET CONFIGURATION
# ============================================================================
datasets:
  sampling_strategy: "round_robin"
  
  nli:
    enabled: true
    source: "Fremtind/all-nli-norwegian"
    languages: ["norwegian"]
    split_ratio: [0.98, 0.01, 0.01]
  
  scandi_qa:
    enabled: true
    use_norquad: true
    use_openbookqa: true
    use_scandiqa: true
    use_supervised_da: true
    use_paws: true
    scandiqa_languages: ["no", "da", "sv"]
    paws_data_dir: "data/paws-x/x-final"
  
  ddsc:
    enabled: true
    languages: ["norwegian", "danish", "swedish"]
    tasks: null
    split_ratio: [0.98, 0.01, 0.01]

# ============================================================================
# EVALUATION CONFIGURATION
# ============================================================================
evaluation:
  evaluator: "sparse_nanobeir"
  nanobeir_datasets: ["msmarco", "nfcorpus", "nq"]
  mteb_tasks: ["NorQuAD", "SNL"]
  track_sparsity: true
  log_decoded_samples: 10

# ============================================================================
# INFERENCE-FREE SPLADE SPECIFIC
# ============================================================================
router:
  mapping:
    anchor: "query"
    positive: "document"
    negative: "document"
  learning_rate_patterns:
    ".*\\.query\\..*": 0.001

# ============================================================================
# MODEL CARD CONFIGURATION
# ============================================================================
model_card:
  language: ["no", "da", "sv"]
  license: "mit"
  library_name: "sentence-transformers"
  tags:
    - "sparse-encoder"
    - "splade"
    - "xlarge"
    - "norwegian"
    - "danish"
    - "swedish"
    - "scandinavian"
    - "multilingual"
    - "information-retrieval"
    - "long-context"
  model_name: "Inference-free SPLADE NorBERT4-XLarge trained on Multi-dataset Norwegian/Scandinavian (Multi-GPU, Long Context)"
  datasets_used:
    - "Fremtind/all-nli-norwegian"
    - "NorQuAD"
    - "OpenBookQA"
    - "ScandiQA"
    - "DDSC Nordic Embedding Training Data"
    - "PAWS-X Norwegian"

# ============================================================================
# HARDWARE CONFIGURATION — MULTI H100 NVL (4-8 GPUs)
# ============================================================================
hardware:
  device: "cuda"
  gradient_checkpointing: false  # Enough VRAM per GPU
  tf32: true
  distributed: true
  world_size: 4  # Change to 8 if using 8 H100s
