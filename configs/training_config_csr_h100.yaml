# CSR (Contrastive Sparse Representation) Training Configuration — H100 GPU Optimized
# Sparsifies existing dense embedding model by adding SparseAutoEncoder
# Uses your finetuned dense model: thivy/norbert4-base-scandinavian-embedding
# Optimized for NVIDIA H100 NVL (96GB VRAM, CUDA 12.2, Compute Capability 9.0)
# ============================================================================
# MODEL CONFIGURATION
# ============================================================================
model:
  base_model: "thivy/norbert4-base-scandinavian-embedding"
  architecture: "csr"
  
  # CSR-specific hyperparameters
  csr_k: 256  # Active dimensions
  csr_k_aux: 512
  csr_hidden_dim: 4096  # 4 * 1024 for norbert4-base
  use_router: false

# ============================================================================
# TRAINING CONFIGURATION
# ============================================================================
training:
  output_dir: "models/csr-norbert4-scandinavian"
  
  # Training duration — CSR trains faster (only training autoencoder)
  num_train_epochs: 1
  
  # Batch sizes — CSR is lightweight, H100 can handle very large batches
  # Only training the autoencoder layer, so memory footprint is small
  per_device_train_batch_size: 128
  per_device_eval_batch_size: 256
  gradient_accumulation_steps: 1
  
  # Learning rate — CSR typically uses higher LR since only training autoencoder
  learning_rate: 0.0001  # 1e-4
  
  # Optimization
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Precision — H100 native BF16 Tensor Cores
  fp16: false
  bf16: true
  
  # Torch compile for H100 kernel optimization
  torch_compile: true
  
  # Sequence length
  max_seq_length: 512
  
  # Batch sampling
  batch_sampler: "no_duplicates"
  
  # Evaluation — more frequent for fast-training CSR
  eval_strategy: "steps"
  eval_steps: 500
  logging_steps: 100
  
  # Saving
  save_strategy: "steps"
  save_steps: 500
  save_total_limit: 1
  load_best_model_at_end: true
  metric_for_best_model: "eval_NanoBEIR_mean_dot_ndcg@10"
  greater_is_better: true
  
  # Push to HuggingFace Hub
  push_to_hub: true
  hub_model_id: "thivy/csr-norbert4-scandinavian"
  hub_strategy: "checkpoint"
  hub_private_repo: false
  
  # Logging
  run_name: "csr-norbert4-scandinavian"
  report_to: ["tensorboard", "mlflow"]
  
  # Data loading — CUDA multi-process data loading
  dataloader_num_workers: 8
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 4

# ============================================================================
# MLFLOW CONFIGURATION
# ============================================================================
mlflow:
  tracking_uri: "mlruns"
  experiment_name: "sparse-encoder-scandinavian"
  tags:
    project: "scandinavian-sparse-embeddings"
    framework: "sentence-transformers"
    architecture: "csr"

# ============================================================================
# LOSS CONFIGURATION
# ============================================================================
loss:
  type: "csr_loss"
  inner_loss: "sparse_multiple_negatives_ranking_loss"
  aux_loss_weight: 0.1

# ============================================================================
# DATASET CONFIGURATION
# ============================================================================
datasets:
  sampling_strategy: "round_robin"
  
  nli:
    enabled: true
    source: "Fremtind/all-nli-norwegian"
    languages: ["norwegian"]
    split_ratio: [0.98, 0.01, 0.01]
  
  scandi_qa:
    enabled: true
    use_norquad: true
    use_openbookqa: true
    use_scandiqa: true
    use_supervised_da: true
    use_paws: true
    scandiqa_languages: ["no", "da", "sv"]
    paws_data_dir: "data/paws-x/x-final"
  
  ddsc:
    enabled: true
    languages: ["norwegian", "danish", "swedish"]
    tasks: null
    split_ratio: [0.98, 0.01, 0.01]

# ============================================================================
# EVALUATION CONFIGURATION
# ============================================================================
evaluation:
  evaluator: "sparse_nanobeir"
  nanobeir_datasets: ["msmarco", "nfcorpus"]
  mteb_tasks: ["NorQuAD", "SNL"]
  track_sparsity: true
  log_decoded_samples: 10

# ============================================================================
# MODEL CARD CONFIGURATION
# ============================================================================
model_card:
  language: ["no", "da", "sv"]
  license: "mit"
  library_name: "sentence-transformers"
  tags:
    - "sparse-encoder"
    - "csr"
    - "contrastive-sparse-representation"
    - "norwegian"
    - "danish"
    - "swedish"
  model_name: "CSR NorBERT4 Scandinavian (sparsified dense model)"
  datasets_used:
    - "Fremtind/all-nli-norwegian"
    - "NorQuAD"
    - "OpenBookQA"
    - "ScandiQA"
    - "DDSC Nordic Embedding Training Data"
    - "PAWS-X Norwegian"

# ============================================================================
# HARDWARE CONFIGURATION — NVIDIA H100 NVL
# ============================================================================
hardware:
  device: "cuda"
  gradient_checkpointing: false  # CSR is lightweight
  tf32: true
  distributed: false
  world_size: 1
